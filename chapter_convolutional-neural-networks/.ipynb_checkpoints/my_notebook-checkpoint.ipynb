{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02b3a88c-c5a6-4d3c-a54d-1b98e054bf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l\n",
    "from torch.nn import functional as F\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a52bd248-8b2c-4f12-9098-fe84ede14325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d(X, K):  #@save\n",
    "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
    "    h, w = K.shape\n",
    "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a170503-bc9c-467d-854d-3b7e93829fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19., 25.],\n",
       "        [37., 43.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "K = torch.tensor([[0.0, 1.0], [2.0, 3.0]])\n",
    "corr2d(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a922bdd-dce2-4459-8f91-ebd4a7f5d921",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, kernel_size):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand(kernel_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return corr2d(x, self.weight) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "61ad9ad8-6a28-470f-bcba-be2ef550fbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.],\n",
       "        [1., 1., 0., 0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.ones((6, 8))\n",
    "X[:, 2:6] = 0\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "05526bfb-bda4-4835-8e3a-2d0a95668f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = torch.tensor([[1.0, -1.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea410286-97ea-4373-aff1-fad63c232ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.,  0., -1.,  0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = corr2d(X, K)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3123274-58af-4e18-bbcf-165215856e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss 4.755\n",
      "epoch 4, loss 0.979\n",
      "epoch 6, loss 0.238\n",
      "epoch 8, loss 0.070\n",
      "epoch 10, loss 0.024\n"
     ]
    }
   ],
   "source": [
    "# Construct a two-dimensional convolutional layer with 1 output channel and a\n",
    "# kernel of shape (1, 2). For the sake of simplicity, we ignore the bias here\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=(1, 2), bias=False)\n",
    "\n",
    "# The two-dimensional convolutional layer uses four-dimensional input and\n",
    "# output in the format of (example, channel, height, width), where the batch\n",
    "# size (number of examples in the batch) and the number of channels are both 1\n",
    "X = X.reshape((1, 1, 6, 8))\n",
    "Y = Y.reshape((1, 1, 6, 7))\n",
    "lr = 3e-2  # Learning rate\n",
    "\n",
    "for i in range(10):\n",
    "    Y_hat = conv2d(X)\n",
    "    l = (Y_hat - Y) ** 2\n",
    "    conv2d.zero_grad()\n",
    "    l.sum().backward()\n",
    "    # Update the kernel\n",
    "    conv2d.weight.data[:] -= lr * conv2d.weight.grad\n",
    "    if (i + 1) % 2 == 0:\n",
    "        print(f'epoch {i + 1}, loss {l.sum():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b9780c2-f4ce-4817-9770-a50b15332a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0046, -0.9746]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d.weight.data.reshape((1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a007d8e-9277-42e9-8dbf-0137081f946f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1876b8c2-3e3b-4fa2-a9d6-e36c0caa6221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n\\n\\n\\n\\nPADDING\\n\\n\\nDimension of a kernel output with padding (ph, pw) is:\\n(nh - kh + ph + 1) x (nw - kw + pw + 1)\\n\\n\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PADDING\n",
    "\n",
    "\n",
    "Dimension of a kernel output with padding (ph, pw) is:\n",
    "(nh - kh + ph + 1) x (nw - kw + pw + 1)\n",
    "\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd832620-4f7c-4b2d-8054-3ca8805f0bf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def comp_conv2d(conv2d, X):\n",
    "    # (1, 1) indicates that batch size and the number of channels are both 1\n",
    "    X = X.reshape((1, 1) + X.shape) # this concatenation, not addition\n",
    "    Y = conv2d(X)\n",
    "    # Strip the first two dimensions: examples and channels\n",
    "    return Y.reshape(Y.shape[2:])\n",
    "\n",
    "conv2d = nn.LazyConv2d(1, kernel_size = 3, padding = 1) \n",
    "X = torch.rand(size=(8, 8))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf4544ac-a539-4ebb-9f9e-d695f8f1ac12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We use a convolution kernel with height 5 and width 3. The padding on either\n",
    "# side of the height and width are 2 and 1, respectively\n",
    "conv2d = nn.LazyConv2d(1, kernel_size=(5, 3), padding=(2, 1)) # (5 - 1) / 2 = 2, (3 - 1) / 2 = 1. => (2, 1) padding\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6dbffd08-fc68-4670-868a-9beeee8289dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\nSTRIDE\\n\\nDimension of a kernel output with padding (ph, pw) and stride (sh, sw) is:\\nfloor((nh - kh + ph)/sh + 1) x floor((nw - kw + pw)/sw + 1) = \\nfloor((nh - kh + ph + sh) / sh) x floor((nw - kw + pw + sw) / sw)\\n\\nif ph = kh - 1 && pw = kw - 1, then dimensions = \\nfloor((nh - 1 + sh) / sh) x floor((nw - 1 + sw) / sw)\\n\\nif nh and nw are divisible by sh and sw respectively, then dimensions = \\n(nh / sh) x (nw / sw)\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "STRIDE\n",
    "\n",
    "Dimension of a kernel output with padding (ph, pw) and stride (sh, sw) is:\n",
    "floor((nh - kh + ph)/sh + 1) x floor((nw - kw + pw)/sw + 1) = \n",
    "floor((nh - kh + ph + sh) / sh) x floor((nw - kw + pw + sw) / sw)\n",
    "\n",
    "if ph = kh - 1 && pw = kw - 1, then dimensions = \n",
    "floor((nh - 1 + sh) / sh) x floor((nw - 1 + sw) / sw)\n",
    "\n",
    "if nh and nw are divisible by sh and sw respectively, then dimensions = \n",
    "(nh / sh) x (nw / sw)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "265030eb-0712-45ef-bb08-24026df5f0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.LazyConv2d(1, kernel_size=3, padding=1, stride=2)\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f44a6dd-2854-4301-af60-fcae16fe61fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv2d = nn.LazyConv2d(1, kernel_size=(3, 5), padding=(0, 1), stride=(3, 4))\n",
    "comp_conv2d(conv2d, X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7ef771-22e6-4274-b9b0-a45956611b38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e0b775c0-80d2-4417-8f22-32d67869edb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\nMultiple input CHANNELS & Multiple output CHANNELS\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Multiple input CHANNELS & Multiple output CHANNELS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9849488a-1727-4efe-a726-21ed876e29b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2d_multi_in(X, K):\n",
    "    # Iterate through the cross-correlation of each channel, then add them up\n",
    "    return sum(d2l.corr2d(x, k) for x, k in zip(X, K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5725ed5e-c1df-4d9b-afed-87036cc1763c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3]) torch.Size([2, 2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  72.],\n",
       "        [104., 120.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]],\n",
    "               [[1.0, 2.0, 3.0], [4.0, 5.0, 6.0], [7.0, 8.0, 9.0]]])\n",
    "K = torch.tensor([[[0.0, 1.0], [2.0, 3.0]], [[1.0, 2.0], [3.0, 4.0]]])\n",
    "print(X.shape, K.shape)\n",
    "corr2d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21b99f83-2b22-4096-9d51-a9dc7198ec79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K is now of shape c_out * c_in * kh * kw\n",
    "# Y is now of shape c_out * out_h * out_w\n",
    "def corr2d_multi_in_out(X, K):\n",
    "    # Iterate through each output channel, and each time, perform\n",
    "    # cross-correlation of the corrseponding conv kernel's output channel with ALL of the channels of input X. \n",
    "    # All of the results are stacked together\n",
    "    return torch.stack([corr2d_multi_in(X, k) for k in K], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23eb68eb-7e2e-43ce-9f9b-389cf6acbaaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[0., 1.],\n",
      "          [2., 3.]],\n",
      "\n",
      "         [[1., 2.],\n",
      "          [3., 4.]]],\n",
      "\n",
      "\n",
      "        [[[1., 2.],\n",
      "          [3., 4.]],\n",
      "\n",
      "         [[2., 3.],\n",
      "          [4., 5.]]],\n",
      "\n",
      "\n",
      "        [[[2., 3.],\n",
      "          [4., 5.]],\n",
      "\n",
      "         [[3., 4.],\n",
      "          [5., 6.]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2, 2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K = torch.stack((K, K + 1, K + 2), 0)\n",
    "print(K)\n",
    "K.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d158d125-0f39-4ddc-8c98-46835ebf3987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 56.,  72.],\n",
      "         [104., 120.]],\n",
      "\n",
      "        [[ 76., 100.],\n",
      "         [148., 172.]],\n",
      "\n",
      "        [[ 96., 128.],\n",
      "         [192., 224.]]])\n",
      "torch.Size([3, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "tmp = corr2d_multi_in_out(X, K)\n",
    "print(tmp)\n",
    "print(tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6915cab7-d8aa-4469-a837-299faae47fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1x1 conv kernel => only compute cross-correlation on the channels of the SAME pixel.\n",
    "def corr2d_multi_in_out_1x1(X, K):\n",
    "    c_i, h, w = X.shape\n",
    "    c_o = K.shape[0]\n",
    "    X = X.reshape((c_i, h * w))  # convert c_in * nh * hw to c_in * (nh*hw), now there are only 2 dimensions.\n",
    "    K = K.reshape((c_o, c_i))    # ignore width and height dim\n",
    "    # Matrix multiplication in the fully connected layer\n",
    "    Y = torch.matmul(K, X)\n",
    "    return Y.reshape((c_o, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1944e45-3fc9-4d04-ab82-d5173cc729b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.normal(0, 1, (3, 3, 3))\n",
    "K = torch.normal(0, 1, (2, 3, 1, 1))\n",
    "Y1 = corr2d_multi_in_out_1x1(X, K)\n",
    "Y2 = corr2d_multi_in_out(X, K)\n",
    "assert float(torch.abs(Y1 - Y2).sum()) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ab90ee-55ac-4ed8-9c87-7c7a1bbf50c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9526c0fd-5759-4a6a-9cc3-e1b5354faf31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\nPOOLING\\n\\nReduces sensitivity to locations, shifts of pixels\\nSimilar sliding to kernels, except it's DETERMINISTIC as it takes NO PARAMETERS\\nApplied AFTER the kernel\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "POOLING\n",
    "\n",
    "Reduces sensitivity to locations, shifts of pixels\n",
    "Similar sliding to kernels, except it's DETERMINISTIC as it takes NO PARAMETERS\n",
    "Applied AFTER the kernel\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "361043f3-121f-48da-aa77-0a3e87e7835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros((X.shape[0] - p_h + 1, X.shape[1] - p_w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i: i + p_h, j: j + p_w].mean()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d4e7e2de-0fb5-4fc1-8801-bc75d290acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 2.],\n",
      "        [3., 4., 5.],\n",
      "        [6., 7., 8.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])\n",
    "print(X)\n",
    "pool2d(X, (2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f123093c-bc14-470f-8bcb-9bdab4477b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2., 3.],\n",
       "        [5., 6.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool2d(X, (2, 2), 'avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1566e4de-e2fa-4410-9295-9d7a4848c35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15.]]]])\n",
      "tensor([[[[10.]]]])\n",
      "tensor([[[[ 5.,  7.],\n",
      "          [13., 15.]]]])\n"
     ]
    }
   ],
   "source": [
    "# using built-in pooling\n",
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "print(X)\n",
    "pool2d = nn.MaxPool2d(3) # window of 3x3, with stride \"of 3x3\n",
    "# Pooling has no model parameters, hence it needs no initialization\n",
    "print(pool2d(X))\n",
    "\n",
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "print(pool2d(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5207cf59-e0ec-418e-a249-49f2e42d9de0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 0.,  1.,  2.,  3.],\n",
      "          [ 4.,  5.,  6.,  7.],\n",
      "          [ 8.,  9., 10., 11.],\n",
      "          [12., 13., 14., 15.]],\n",
      "\n",
      "         [[ 1.,  2.,  3.,  4.],\n",
      "          [ 5.,  6.,  7.,  8.],\n",
      "          [ 9., 10., 11., 12.],\n",
      "          [13., 14., 15., 16.]]]])\n",
      "torch.Size([1, 2, 4, 4])\n",
      "\n",
      "tensor([[[[ 5.,  7.],\n",
      "          [13., 15.]],\n",
      "\n",
      "         [[ 6.,  8.],\n",
      "          [14., 16.]]]])\n",
      "torch.Size([1, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "# pooling applies to each channel separately, instead of summing like in kernels\n",
    "# => pooling layer has same c_out as c_in\n",
    "X = torch.arange(16, dtype=torch.float32).reshape((1, 1, 4, 4))\n",
    "X = torch.cat((X, X + 1), 1)\n",
    "print(X)\n",
    "print(X.shape)\n",
    "\n",
    "pool2d = nn.MaxPool2d(3, padding=1, stride=2)\n",
    "print()\n",
    "print(pool2d(X))\n",
    "print(pool2d(X).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cacef8c-d064-4868-b305-dea97dd0f133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0fd60622-b1b3-4ce8-8348-52bf69a6567d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\n\\nConvolutional Neural Networks (LeNet)\\n\\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Convolutional Neural Networks (LeNet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6252d608-d660-465a-acfc-bfb754680f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_cnn(module):  #@save\n",
    "    \"\"\"Initialize weights for CNNs.\"\"\"\n",
    "    if type(module) == nn.Linear or type(module) == nn.Conv2d:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "class LeNet(d2l.Classifier):  #@save\n",
    "    \"\"\"The LeNet-5 model.\"\"\"\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
    "            nn.Flatten(),\n",
    "            nn.LazyLinear(120), nn.Sigmoid(),\n",
    "            nn.LazyLinear(84), nn.Sigmoid(),\n",
    "            nn.LazyLinear(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "be5e376b-d652-4f29-95b3-ed42d47a659c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d output shape:\t torch.Size([1, 6, 28, 28])\n",
      "Sigmoid output shape:\t torch.Size([1, 6, 28, 28])\n",
      "AvgPool2d output shape:\t torch.Size([1, 6, 14, 14])\n",
      "Conv2d output shape:\t torch.Size([1, 16, 10, 10])\n",
      "Sigmoid output shape:\t torch.Size([1, 16, 10, 10])\n",
      "AvgPool2d output shape:\t torch.Size([1, 16, 5, 5])\n",
      "Flatten output shape:\t torch.Size([1, 400])\n",
      "Linear output shape:\t torch.Size([1, 120])\n",
      "Sigmoid output shape:\t torch.Size([1, 120])\n",
      "Linear output shape:\t torch.Size([1, 84])\n",
      "Sigmoid output shape:\t torch.Size([1, 84])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "@d2l.add_to_class(d2l.Classifier)  #@save\n",
    "def layer_summary(self, X_shape):\n",
    "    X = torch.randn(*X_shape)\n",
    "    for layer in self.net:\n",
    "        X = layer(X)\n",
    "        print(layer.__class__.__name__, 'output shape:\\t', X.shape)\n",
    "\n",
    "model = LeNet()\n",
    "model.layer_summary((1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6a83ab69-de3c-4b78-a512-eb5dfa34a053",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "data = d2l.FashionMNIST(batch_size=128)\n",
    "model = LeNet(lr = 0.1)\n",
    "#model.apply_init([next(iter(data.get_dataloader(True)))[0]], init_cnn)\n",
    "#trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf361b7-76cb-459d-9ad4-c534973ef275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1dff122-e94b-440d-8887-0b08bbff8178",
   "metadata": {},
   "source": [
    "# Networks in Networks (NiN)\n",
    "use 1x1 conv layers to add non-linearities\n",
    "then use global avg pooling at the end to replace fully connected dense layers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "816b92c1-c299-42e0-b8f1-c352158a2f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 96, 54, 54])\n",
      "MaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\n",
      "Sequential output shape:\t torch.Size([1, 256, 26, 26])\n",
      "MaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 384, 12, 12])\n",
      "MaxPool2d output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Dropout output shape:\t torch.Size([1, 384, 5, 5])\n",
      "Sequential output shape:\t torch.Size([1, 10, 5, 5])\n",
      "AdaptiveAvgPool2d output shape:\t torch.Size([1, 10, 1, 1])\n",
      "Flatten output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "def nin_block(out_channels, kernel_size, strides, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.LazyConv2d(out_channels, kernel_size, strides, padding), nn.ReLU(),\n",
    "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU(),\n",
    "        nn.LazyConv2d(out_channels, kernel_size=1), nn.ReLU())\n",
    "\n",
    "class NiN(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(\n",
    "            nin_block(96, kernel_size=11, strides=4, padding=0),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nin_block(256, kernel_size=5, strides=1, padding=2),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nin_block(384, kernel_size=3, strides=1, padding=1),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            nn.Dropout(0.5),\n",
    "            nin_block(num_classes, kernel_size=3, strides=1, padding=1),\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten())\n",
    "        self.net.apply(d2l.init_cnn)\n",
    "        \n",
    "NiN().layer_summary((1, 1, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c10784-2d98-4f0d-9362-53f134ca30aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fb365f56-c565-4e45-8da2-2ec5530577e2",
   "metadata": {},
   "source": [
    "# Multi-brach networks (GoogLeNET)\n",
    "do parallel convs, then concat them along channel dim to capture more details among different filter sizes\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4df062-8a61-45bb-8caa-92863d7082f1",
   "metadata": {},
   "source": [
    "![Structure of the Inception block.](../img/inception.svg)\n",
    ":label:`fig_inception`\n",
    "\n",
    "![The GoogLeNet architecture.](../img/inception-full-90.svg)\n",
    ":label:`fig_inception_full`\n",
    "\n",
    "![GoogLeNet dimensions.](https://discuss.d2l.ai/uploads/default/original/2X/1/19f3b19af89bb820949a7bdb21328ad1cd07d1b8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a65ad791-47bc-4326-b38f-45e44c8be4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Inception blocks are new compared to previous models, this is where multi-branch conv happens \"\"\"\n",
    "class Inception(nn.Module):\n",
    "    # c1--c4 are the number of output channels for each branch\n",
    "    def __init__(self, c1, c2, c3, c4, **kwargs):\n",
    "        super(Inception, self).__init__(**kwargs)\n",
    "        # Branch 1\n",
    "        self.b1_1 = nn.LazyConv2d(c1, kernel_size=1)\n",
    "        # Branch 2\n",
    "        self.b2_1 = nn.LazyConv2d(c2[0], kernel_size=1)\n",
    "        self.b2_2 = nn.LazyConv2d(c2[1], kernel_size=3, padding=1)\n",
    "        # Branch 3\n",
    "        self.b3_1 = nn.LazyConv2d(c3[0], kernel_size=1)\n",
    "        self.b3_2 = nn.LazyConv2d(c3[1], kernel_size=5, padding=2)\n",
    "        # Branch 4\n",
    "        self.b4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
    "        self.b4_2 = nn.LazyConv2d(c4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = F.relu(self.b1_1(x))\n",
    "        b2 = F.relu(self.b2_2(F.relu(self.b2_1(x))))\n",
    "        b3 = F.relu(self.b3_2(F.relu(self.b3_1(x))))\n",
    "        b4 = F.relu(self.b4_2(self.b4_1(x)))\n",
    "        return torch.cat((b1, b2, b3, b4), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b411da2a-00de-469b-9aa3-3d3fc6cd57ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogleNet(d2l.Classifier):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super(GoogleNet, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(self.b1(), self.b2(), self.b3(), self.b4(),\n",
    "                                 self.b5(), nn.LazyLinear(num_classes))\n",
    "        self.net.apply(d2l.init_cnn)\n",
    "    \n",
    "    # 64-out-channel 7x7 conv layer -> max pool 3x3\n",
    "    def b1(self):\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.ReLU(), nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        \n",
    "    # 64-out-channel 1x1 conv -> 192-out-channel 3x3 conv -> max pool 3x3 -> feed into Inception blocks\n",
    "    def b2(self):\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=1), nn.ReLU(),\n",
    "            nn.LazyConv2d(192, kernel_size=3, padding=1), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "        \n",
    "    # 2 Inception blocks -> max pool 3x3 \n",
    "    # first block has 64 + 128 + 32 + 32 = 256 channels => ratio of channels of branches are 2:4:1:1\n",
    "    # TODO: find out why the # of input channels for 2nd and 3rd branches are reduced? \n",
    "    # possibly because 3x3 and 5x5 are expensive => reduce complexity and overfitting.\n",
    "    def b3(self):\n",
    "        return nn.Sequential(Inception(64, (96, 128), (16, 32), 32),\n",
    "                         Inception(128, (128, 192), (32, 96), 64),\n",
    "                         nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "    # 5 Inception blocks -> 832 out channels -> max pool 3x3 -> 832x3x3\n",
    "    def b4(self):\n",
    "        return nn.Sequential(Inception(192, (96, 208), (16, 48), 64),\n",
    "                             Inception(160, (112, 224), (24, 64), 64),\n",
    "                             Inception(128, (128, 256), (24, 64), 64),\n",
    "                             Inception(112, (144, 288), (32, 64), 64),\n",
    "                             Inception(256, (160, 320), (32, 128), 128),\n",
    "                             nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "    # 2 Inception blocks -> 1024 out channels-> 1024x3x3 -> adaptive avg pool -> 1024x1x1\n",
    "    # adaptive avg pool: automatically infer kernel size and stride based on inp/output size (https://stackoverflow.com/questions/58692476/what-is-adaptive-average-pooling-and-how-does-it-work)\n",
    "    def b5(self):\n",
    "        return nn.Sequential(Inception(256, (160, 320), (32, 128), 128),\n",
    "                             Inception(384, (192, 384), (48, 128), 128),\n",
    "                             nn.AdaptiveAvgPool2d((1,1)), nn.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42580f08-3a27-459a-b54f-08758f220ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
      "Sequential output shape:\t torch.Size([1, 192, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 480, 6, 6])\n",
      "Sequential output shape:\t torch.Size([1, 832, 3, 3])\n",
      "Sequential output shape:\t torch.Size([1, 1024])\n",
      "Linear output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
    "model = GoogleNet().layer_summary((1, 1, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6314d822-36bf-481b-921c-d773467ba435",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GoogleNet(lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
    "#model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "#trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745c7be8-aeb9-4f63-b1d1-869a7df62f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "386d4404-bf3f-4d53-bf89-dfb0ed1af5ba",
   "metadata": {},
   "source": [
    "# Batch Normalization\n",
    "\n",
    "## Fully Connected Layer Normalization\n",
    "Denote by $\\mathcal{B}$ a minibatch and let $\\mathbf{x} \\in \\mathcal{B}$ be an input to \n",
    "batch normalization ($\\textrm{BN}$). \n",
    "$$\\textrm{BN}(\\mathbf{x}) = \\boldsymbol{\\gamma} \\odot \\frac{\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_\\mathcal{B}}{\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}} + \\boldsymbol{\\beta}.$$\n",
    "\n",
    "\n",
    "for *scale parameter* is $\\boldsymbol{\\gamma}$ and *shift parameter* is $\\boldsymbol{\\beta}$.\n",
    "We calculate $\\hat{\\boldsymbol{\\mu}}_\\mathcal{B}$ and ${\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}}$ as follows:\n",
    "\n",
    "$$\\hat{\\boldsymbol{\\mu}}_\\mathcal{B} = \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} \\mathbf{x}\n",
    "\\textrm{ and }\n",
    "\\hat{\\boldsymbol{\\sigma}}_\\mathcal{B}^2 = \\frac{1}{|\\mathcal{B}|} \\sum_{\\mathbf{x} \\in \\mathcal{B}} (\\mathbf{x} - \\hat{\\boldsymbol{\\mu}}_{\\mathcal{B}})^2 + \\epsilon.$$\n",
    "\n",
    "$\\epsilon$ introduces noise and prevents division by 0. This noise actually **improves** generalization as seen in previous models/algorithms as well.\n",
    "\n",
    "## Conv Layer Normalization\n",
    "Similar formula to above, except normalization is applied **after** *conv* layer and **before** *activation*. \n",
    "\n",
    "Assume that our minibatches contain $m$ examples\n",
    "and that for each *output channel*,\n",
    "the output of the convolution has height $p$ and width $q$.\n",
    "For convolutional layers, we carry out each batch normalization\n",
    "over the $m \\cdot p \\cdot q$ elements per *output channel*\n",
    "\n",
    "=> Normalization over **all locations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "85a5b546-a4a8-4ff8-8765-9cf677941064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_norm(X, gamma, beta, moving_mean, moving_var, eps, momentum):\n",
    "    # Use is_grad_enabled to determine whether we are in training mode\n",
    "    if not torch.is_grad_enabled():\n",
    "        # In prediction mode, use mean and variance obtained by moving average\n",
    "        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)\n",
    "    else:\n",
    "        assert len(X.shape) in (2, 4)\n",
    "        if len(X.shape) == 2:\n",
    "            # When using a fully connected layer, calculate the mean and\n",
    "            # variance on the feature dimension\n",
    "            mean = X.mean(dim=0)\n",
    "            var = ((X - mean) ** 2).mean(dim=0)\n",
    "        else:\n",
    "            # When using a two-dimensional convolutional layer, calculate the\n",
    "            # mean and variance on the channel dimension (axis=1). Here we\n",
    "            # need to maintain the shape of X, so that the broadcasting\n",
    "            # operation can be carried out later\n",
    "            mean = X.mean(dim=(0, 2, 3), keepdim=True)\n",
    "            var = ((X - mean) ** 2).mean(dim=(0, 2, 3), keepdim=True)\n",
    "        # In training mode, the current mean and variance are used\n",
    "        X_hat = (X - mean) / torch.sqrt(var + eps)\n",
    "        # Update the mean and variance using moving average\n",
    "        moving_mean = (1.0 - momentum) * moving_mean + momentum * mean\n",
    "        moving_var = (1.0 - momentum) * moving_var + momentum * var\n",
    "    Y = gamma * X_hat + beta  # Scale and shift\n",
    "    return Y, moving_mean.data, moving_var.data\n",
    "\n",
    "class BatchNorm(nn.Module):\n",
    "    # num_features: the number of outputs for a fully connected layer or the\n",
    "    # number of output channels for a convolutional layer. num_dims: 2 for a\n",
    "    # fully connected layer and 4 for a convolutional layer\n",
    "    def __init__(self, num_features, num_dims):\n",
    "        super().__init__()\n",
    "        if num_dims == 2:\n",
    "            shape = (1, num_features)\n",
    "        else:\n",
    "            shape = (1, num_features, 1, 1)\n",
    "        # The scale parameter and the shift parameter (model parameters) are\n",
    "        # initialized to 1 and 0, respectively\n",
    "        self.gamma = nn.Parameter(torch.ones(shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(shape))\n",
    "        # The variables that are not model parameters are initialized to 0 and\n",
    "        # 1\n",
    "        self.moving_mean = torch.zeros(shape)\n",
    "        self.moving_var = torch.ones(shape)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # If X is not on the main memory, copy moving_mean and moving_var to\n",
    "        # the device where X is located\n",
    "        if self.moving_mean.device != X.device:\n",
    "            self.moving_mean = self.moving_mean.to(X.device)\n",
    "            self.moving_var = self.moving_var.to(X.device)\n",
    "        # Save the updated moving_mean and moving_var\n",
    "        Y, self.moving_mean, self.moving_var = batch_norm(\n",
    "            X, self.gamma, self.beta, self.moving_mean,\n",
    "            self.moving_var, eps=1e-5, momentum=0.1)\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1174f19-d6c5-4918-abc2-6500f8b613f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ad5fe9e-76f6-4530-b4dc-5c50d2fb04fc",
   "metadata": {},
   "source": [
    "# Residual Network (ResNET)\n",
    "\n",
    "Consider $\\mathcal{F}$, the class of functions that a specific network architecture (together with learning rates and other hyperparameter settings) can reach.\n",
    "That is, for all $f \\in \\mathcal{F}$ there exists some set of parameters (e.g., weights and biases) that can be obtained through training on a suitable dataset.\n",
    "Let's assume that $f^*$ is the \"truth\" function that we really would like to find.\n",
    "If it is in $\\mathcal{F}$, we are in good shape but typically we will not be quite so lucky.\n",
    "Instead, we will try to find some $f^*_\\mathcal{F}$ which is our best bet within $\\mathcal{F}$.\n",
    "For instance,\n",
    "given a dataset with features $\\mathbf{X}$\n",
    "and labels $\\mathbf{y}$,\n",
    "we might try finding it by solving the following optimization problem:\n",
    "\n",
    "$$f^*_\\mathcal{F} \\stackrel{\\textrm{def}}{=} \\mathop{\\mathrm{argmin}}_f L(\\mathbf{X}, \\mathbf{y}, f) \\textrm{ subject to } f \\in \\mathcal{F}.$$\n",
    "\n",
    "![For non-nested function classes, a larger (indicated by area) function class does not guarantee we will get closer to the \"truth\" function ($\\mathit{f}^*$). This does not happen in nested function classes.](../img/functionclasses.svg)\n",
    ":label:`fig_functionclasses`\n",
    "\n",
    "\n",
    "For deep neural networks,\n",
    "if we can\n",
    "train the newly-added layer into an identity function $f(\\mathbf{x}) = \\mathbf{x}$, the new model will be as effective as the original model. After all, the default behavior of a layer is to let the input pass through unchanged.\n",
    "\n",
    "As the new model may get a better solution to fit the training dataset, the added layer might make it easier to reduce training errors.\n",
    "\n",
    "**Residual network** (*ResNet*) is the idea that every additional layer should more easily contain the **identity function** as one of its elements.\n",
    "\n",
    "On the left below,\n",
    "the portion within the dotted-line box\n",
    "must directly learn $f(\\mathbf{x})$.\n",
    "On the right,\n",
    "the portion within the dotted-line box\n",
    "needs to\n",
    "learn the *residual mapping* $g(\\mathbf{x}) = f(\\mathbf{x}) - \\mathbf{x}$,\n",
    "which is how the residual block derives its name.\n",
    "If the identity mapping $f(\\mathbf{x}) = \\mathbf{x}$ is the desired underlying mapping,\n",
    "the residual mapping amounts to $g(\\mathbf{x}) = 0$ and it is thus easier to learn:\n",
    "we only need to push the weights and biases\n",
    "of the\n",
    "upper weight layer (e.g., fully connected layer and convolutional layer)\n",
    "within the dotted-line box\n",
    "to zero.\n",
    "The right figure illustrates the *residual block* of ResNet,\n",
    "where the solid line carrying the layer input\n",
    "$\\mathbf{x}$ to the addition operator\n",
    "is called a *residual connection* (or *shortcut connection*).\n",
    "\n",
    "\n",
    "\n",
    "![In a regular block (left), the portion within the dotted-line box must directly learn the mapping $\\mathit{f}(\\mathbf{x})$. In a residual block (right), the portion within the dotted-line box needs to learn the residual mapping $\\mathit{g}(\\mathbf{x}) = \\mathit{f}(\\mathbf{x}) - \\mathbf{x}$, making the identity mapping $\\mathit{f}(\\mathbf{x}) = \\mathbf{x}$ easier to learn.](../img/residual-block.svg)\n",
    ":label:`fig_residual_block`\n",
    "\n",
    "\n",
    "=> Two-branch Inception block\n",
    "\n",
    "\n",
    "Full architecture of ResNet-18: \n",
    "\n",
    "![The ResNet-18 architecture.](../img/resnet18-90.svg)\n",
    ":label:`fig_resnet18`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "73e4609f-959c-497a-90f6-945c21f35ec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# example of keeping # of out channels\\nblk = Residual(3)\\nX = torch.randn(4, 3, 6, 6) # 4 is the number of examples\\nblk(X).shape\\n\\n# example of increasing # of out channels\\nblk = Residual(6, use_1x1conv=True, strides=2)\\nblk(X).shape\\n'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Residual(nn.Module):  #@save\n",
    "    \"\"\"The Residual block of ResNet models.\n",
    "    The residual layers have the same # of out channels\n",
    "    `strides` reduces the spatial dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, num_channels, use_1x1conv=False, strides=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1,\n",
    "                                   stride=strides)\n",
    "        self.conv2 = nn.LazyConv2d(num_channels, kernel_size=3, padding=1)\n",
    "        \n",
    "        if use_1x1conv: \n",
    "            self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "            \n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        Y += X\n",
    "        return F.relu(Y)\n",
    "\n",
    "\n",
    "class ResNet(d2l.Classifier):\n",
    "    \"\"\"\n",
    "        `arch`: tuple of (num_res, num_channels) tuples for the `block` function\n",
    "    \"\"\"\n",
    "    def __init__(self, arch, lr=0.1, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(self.b1())\n",
    "        for i, b in enumerate(arch):\n",
    "            self.net.add_module(f'b{i+2}', self.block(*b, first_block=(i==0)))\n",
    "        self.net.add_module('last', nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "            nn.LazyLinear(num_classes)))\n",
    "        self.net.apply(d2l.init_cnn)\n",
    "    \n",
    "    def b1(self):\n",
    "        return nn.Sequential(\n",
    "            nn.LazyConv2d(64, kernel_size=7, stride=2, padding=3),\n",
    "            nn.LazyBatchNorm2d(), nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n",
    "\n",
    "    def block(self, num_residuals, num_channels, first_block=False):\n",
    "        blk = []\n",
    "        for i in range(num_residuals):\n",
    "            if i == 0 and not first_block:\n",
    "                blk.append(Residual(num_channels, use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(Residual(num_channels))\n",
    "        return nn.Sequential(*blk)\n",
    "\n",
    "\"\"\"\n",
    "# example of keeping # of out channels\n",
    "blk = Residual(3)\n",
    "X = torch.randn(4, 3, 6, 6) # 4 is the number of examples\n",
    "blk(X).shape\n",
    "\n",
    "# example of increasing # of out channels\n",
    "blk = Residual(6, use_1x1conv=True, strides=2)\n",
    "blk(X).shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5a0336aa-76a1-48d2-9c94-7f6691aa4991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
      "Sequential output shape:\t torch.Size([1, 64, 24, 24])\n",
      "Sequential output shape:\t torch.Size([1, 128, 12, 12])\n",
      "Sequential output shape:\t torch.Size([1, 256, 6, 6])\n",
      "Sequential output shape:\t torch.Size([1, 512, 3, 3])\n",
      "Sequential output shape:\t torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "class ResNet18(ResNet):\n",
    "    def __init__(self, lr=0.1, num_classes=10):\n",
    "        super().__init__(((2, 64), (2, 128), (2, 256), (2, 512)),\n",
    "                       lr, num_classes)\n",
    "\n",
    "ResNet18().layer_summary((1, 1, 96, 96))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "89795f45-2dd6-48cb-8ada-f6829000cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18(lr=0.01)\n",
    "trainer = d2l.Trainer(max_epochs=10, num_gpus=1)\n",
    "data = d2l.FashionMNIST(batch_size=128, resize=(96, 96))\n",
    "#model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "#trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed426fab-6df2-4716-9acc-5b18940cb58d",
   "metadata": {},
   "source": [
    "## Group Convolution\n",
    "\n",
    "Breaking up a convolution from $c_\\textrm{i}$ to $c_\\textrm{o}$ channels into one of $g$ groups of size $c_\\textrm{i}/g$ generating $g$ outputs of size $c_\\textrm{o}/g$ is called a *grouped convolution*. The **computational cost** (proportionally) is **reduced** from $\\mathcal{O}(c_\\textrm{i} \\cdot c_\\textrm{o})$ to $\\mathcal{O}(g \\cdot (c_\\textrm{i}/g) \\cdot (c_\\textrm{o}/g)) = \\mathcal{O}(c_\\textrm{i} \\cdot c_\\textrm{o} / g)$, i.e., it is $g$ times faster. Even better, the **number of parameters** needed to generate the output is also **reduced** from a $c_\\textrm{i} \\times c_\\textrm{o}$ matrix to $g$ smaller matrices of size $(c_\\textrm{i}/g) \\times (c_\\textrm{o}/g)$, again a $g$ times reduction. In what follows we assume that both $c_\\textrm{i}$ and $c_\\textrm{o}$ are divisible by $g$. \n",
    "\n",
    "\n",
    "![The ResNeXt block. The use of grouped convolution with $\\mathit{g}$ groups is $\\mathit{g}$ times faster than a dense convolution. It is a bottleneck residual block when the number of intermediate channels $\\mathit{b}$ is less than $\\mathit{c}$.](../img/resnext-block.svg)\n",
    ":label:`fig_resnext_block`\n",
    "\n",
    "The only challenge in this design is that no information is exchanged between the $g$ groups. The ResNeXt block of \n",
    ":numref:`fig_resnext_block` fixes this in two ways: the grouped convolution with a $3 \\times 3$ kernel is sandwiched in between two $1 \\times 1$ convolutions. The second one serves double duty in changing the number of channels back. In the case of **ResNeXtBlock**, $c_\\textrm{o} == c_\\textrm{i}$. The benefit is that we only pay the $\\mathcal{O}(c_i \\cdot b)$ and $\\mathcal{O}(c_o \\cdot b)$ cost for $1 \\times 1$ kernels and can make do with an $\\mathcal{O}(b^2 / g)$ cost for $3 \\times 3$ kernels. Similar to the residual block implementation in\n",
    ":numref:`subsec_residual-blks`, the residual connection is replaced (thus generalized) by a $1 \\times 1$ convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d7cb4d2-afc9-4c21-af94-21bfc1b9cb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNeXtBlock(nn.Module):  #@save\n",
    "    \"\"\"The ResNeXt block.\"\"\"\n",
    "    def __init__(self, num_channels, groups, bot_mul, use_1x1conv=False,\n",
    "                 strides=1):\n",
    "        super().__init__()\n",
    "        bot_channels = int(round(num_channels * bot_mul))\n",
    "        self.conv1 = nn.LazyConv2d(bot_channels, kernel_size=1, stride=1)\n",
    "        \n",
    "        self.conv2 = nn.LazyConv2d(bot_channels, kernel_size=3,\n",
    "                                   stride=strides, padding=1,\n",
    "                                   groups=bot_channels//groups)\n",
    "        \n",
    "        self.conv3 = nn.LazyConv2d(num_channels, kernel_size=1, stride=1)\n",
    "        self.bn1 = nn.LazyBatchNorm2d()\n",
    "        self.bn2 = nn.LazyBatchNorm2d()\n",
    "        self.bn3 = nn.LazyBatchNorm2d()\n",
    "        if use_1x1conv:\n",
    "            self.conv4 = nn.LazyConv2d(num_channels, kernel_size=1,\n",
    "                                       stride=strides)\n",
    "            self.bn4 = nn.LazyBatchNorm2d()\n",
    "        else:\n",
    "            self.conv4 = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = F.relu(self.bn2(self.conv2(Y)))\n",
    "        Y = self.bn3(self.conv3(Y))\n",
    "        if self.conv4:\n",
    "            X = self.bn4(self.conv4(X))\n",
    "        return F.relu(Y + X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602fbd63-7b19-4fea-af38-7a3e3be2c43e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c973bf17-3016-47d9-badb-e8bb44d272d5",
   "metadata": {},
   "source": [
    "# Designing Convolutional Network Architecture\n",
    "\n",
    "![The AnyNet design space. The numbers $(\\mathit{c}, \\mathit{r})$ along each arrow indicate the number of channels $c$ and the resolution $\\mathit{r} \\times \\mathit{r}$ of the images at that point. From left to right: generic network structure composed of stem, body, and head; body composed of four stages; detailed structure of a stage; two alternative structures for blocks, one without downsampling and one that halves the resolution in each dimension. Design choices include depth $\\mathit{d_i}$, the number of output channels $\\mathit{c_i}$, the number of groups $\\mathit{g_i}$, and bottleneck ratio $\\mathit{k_i}$ for any stage $\\mathit{i}$.](../img/anynet.svg)\n",
    ":label:`fig_anynet_full`\n",
    "\n",
    "Let's review the structure outlined in :numref:`fig_anynet_full` in detail. As mentioned, an AnyNet consists of a stem, body, and head. The stem takes as its input RGB images (3 channels), using a $3 \\times 3$ convolution with a stride of $2$, followed by a batch norm, to halve the resolution from $r \\times r$ to $r/2 \\times r/2$. Moreover, it generates $c_0$ channels that serve as input to the body. \n",
    "\n",
    "Since the network is designed to work well with ImageNet images of shape $224 \\times 224 \\times 3$, the body serves to reduce this to $7 \\times 7 \\times c_4$ through 4 stages (recall that $224 / 2^{1+4} = 7$), each with an eventual stride of $2$. Lastly, the head employs an entirely standard design via global average pooling, similar to NiN (:numref:`sec_nin`), followed by a fully connected layer to emit an $n$-dimensional vector for $n$-class classification. \n",
    "\n",
    "Most of the relevant design decisions are inherent to the body of the network. It proceeds in stages, where each stage is composed of the same type of ResNeXt blocks as we discussed in :numref:`subsec_resnext`. The design there is again entirely generic: we begin with a block that halves the resolution by using a stride of $2$ (the rightmost in :numref:`fig_anynet_full`). To match this, the residual branch of the ResNeXt block needs to pass through a $1 \\times 1$ convolution. This block is followed by a variable number of additional ResNeXt blocks that leave both resolution and the number of channels unchanged. Note that a common design practice is to add a slight bottleneck in the design of convolutional blocks. \n",
    "As such, with bottleneck ratio $k_i \\geq 1$ we afford some number of channels, $c_i/k_i$,  within each block for stage $i$ (as the experiments show, this is not really effective and should be skipped). Lastly, since we are dealing with ResNeXt blocks, we also need to pick the number of groups $g_i$ for grouped convolutions at stage $i$. \n",
    "\n",
    "This seemingly **generic design** space provides us nonetheless with **many parameters**: we can set the block width (number of channels) $c_0, \\ldots c_4$, the depth (number of blocks) per stage $d_1, \\ldots d_4$, the bottleneck ratios $k_1, \\ldots k_4$, and the group widths (numbers of groups) $g_1, \\ldots g_4$. \n",
    "In total this adds up to 17 parameters, resulting in an unreasonably large number of configurations that would warrant exploring. We need some tools to reduce this huge design space effectively. This is where the conceptual beauty of design spaces comes in. Before we do so, let's implement the generic design first.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c7f32b-3f38-4966-89a3-7ffdde0585a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnyNet(d2l.Classifier):\n",
    "    def __init__(self, arch, stem_channels, lr=0.1, num_classes=10):\n",
    "        super(AnyNet, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        # stem\n",
    "        self.net = nn.Sequential(self.stem(stem_channels)) \n",
    "        # body\n",
    "        for i, s in enumerate(arch):\n",
    "            self.net.add_module(f'stage{i+1}', self.stage(*s))\n",
    "        # head\n",
    "        self.net.add_module('head', nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(),\n",
    "            nn.LazyLinear(num_classes)))\n",
    "        self.net.apply(d2l.init_cnn)\n",
    "    \n",
    "    def stem(self, num_channels):\n",
    "        \"\"\"Halve dim with c0 out channels\"\"\"\n",
    "        return nn.Sequential(nn.LazyConv2d(num_channels, kernel_size=3, padding=1, strides=2),\n",
    "                             nn.LazyBatchNorm2d(), nn.ReLU())\n",
    "\n",
    "    def stage(self, depth, num_channels, groups, bot_mul):\n",
    "        blk = []\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                blk.append(ResNeXtBlock(num_channels, groups, bot_mul,\n",
    "                            use_1x1conv=True, strides=2))\n",
    "            else:\n",
    "                blk.append(ResNeXtBlock(num_channels, groups, bot_mul))\n",
    "        return nn.Sequential(*blk)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fb5c67-ae41-47cb-909d-71a098b4d456",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:d2l] *",
   "language": "python",
   "name": "conda-env-d2l-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
